{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/cristobalchavez/xgboost-drop-columns-and-tuning-with-optuna?scriptVersionId=125561649\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_log_error,  make_scorer, roc_auc_score\nprepath = \"\"\ntrainpath = prepath+\"/kaggle/input/playground-series-s3e11/train.csv\"\ntestpath = prepath+\"/kaggle/input/playground-series-s3e11/test.csv\"\noriginalpath = prepath+\"/kaggle/input/media-campaign-cost-prediction/train_dataset.csv\"\noutputpath = prepath+\"/kaggle/working/playground-series-s3e11/\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(trainpath)\ndata = data.drop(columns=[\"id\"])\ndata.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_target = data.columns\nfeatures = list(features_target[:-1])\n\ncorr = data[features_target].corr()\n\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\nf, ax = plt.subplots(figsize=(11, 9))\n\nsns.heatmap(corr, mask=mask, cmap=\"coolwarm\", \n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Data from original dataset\noriginal_df = pd.read_csv(originalpath)\n# original_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train test split\nx_train, x_test, y_train, y_test = train_test_split(\n    data[features], data[\"cost\"], test_size=0.30, random_state=21)\n# Add original dataset to training set\ntrain = pd.concat([x_train, y_train], axis=1)\ntrain2 = pd.concat([train, original_df])\n# Shuffle\ntrain2 = train2.sample(frac=1)\nx_train_2 = train2[features]\ny_train_2 = train2[\"cost\"]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# XGBoost default parameters\nreg_default = xgb.XGBRegressor(n_estimators=1000,\n                            early_stopping_rounds=20, \n                            eval_metric=\"rmsle\",\n                            verbosity=0\n                         )\nreg_default.fit(x_train, y_train, eval_set=[(x_test, y_test)])\nprint(reg_default)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_test_default = reg_default.predict(x_test).flatten()\n#logloss of the model trained with original data\nprint(mean_squared_log_error(y_test, y_pred_test_default, squared=False)\n)\n# xgb.plot_importance(reg_default, importance_type='weight')\nxgb.plot_importance(reg_default, importance_type='gain')\n# xgb.plot_importance(reg_default, importance_type='cover')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.plot_importance(reg_default, importance_type='cover')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Best parameters from previous notebook\nparams = { \"n_estimators\": 10000,\n            \"max_depth\": 11,\n            \"learning_rate\": 0.01,\n            \"gamma\": 22,\n            \"min_child_weight\": 20,\n            \"reg_lambda\": 0,\n            \"eval_metric\": \"rmsle\",\n            \"early_stopping_rounds\": 20,\n            \"objective\":\"reg:squarederror\",\n            \"verbosity\": 1,\n            \"subsample\": 0.8\n            }\n\nreg_prev = xgb.XGBRegressor(**params)\nreg_prev.fit(x_train, y_train, eval_set=[(x_test, y_test)])\nprint(reg_prev)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_test_prev = reg_prev.predict(x_test).flatten()\n#logloss of the model trained with original data\nmean_squared_log_error(y_test, y_pred_test_prev, squared=False)\n# xgb.plot_importance(reg_prev, importance_type='weight')\nxgb.plot_importance(reg_prev, importance_type='gain')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgb.plot_importance(reg_prev, importance_type='cover')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drop features with low average gain\ndropping = [\n            \"low_fat\",\n            \"recyclable_package\",\n            \"units_per_case\",\n            \"gross_weight\",\n            \"store_sales(in millions)\",\n            \"unit_sales(in millions)\"\n]\nx_train_dropped = x_train.drop(columns=dropping, inplace=False)\nx_test_dropped = x_test.drop(columns=dropping, inplace=False)\nx_train_2_dropped = x_train_2.drop(columns=dropping, inplace=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find good parameters with optuna and original+generated data\nimport optuna\nfrom sklearn.model_selection import cross_val_score\nrmsle=make_scorer(mean_squared_log_error, greater_is_better=False, squared=False)\ndef objective(trial):\n\n    param = {\n        \"objective\": 'reg:squarederror',\n        \"eval_metric\": \"rmsle\",\n        'tree_method': 'gpu_hist',\n        \"n_estimators\": 1000,\n        \"early_stopping_rounds\":10,\n        \"verbosity\": 0,\n        \"reg_lambda\": trial.suggest_float(\"lambda\", 1e-5, 100),\n        \"reg_alpha\": trial.suggest_loguniform(\"alpha\", 1e-5, 100),\n        \"max_depth\":  trial.suggest_int(\"max_depth\", 3, 20),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.0001, 0.5),\n        \"gamma\": trial.suggest_float(\"gamma\", 0, 30),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 150),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0)\n    }\n\n    fit_params = {\n                \"eval_set\":[(x_test_dropped, y_test)],\n                \"verbose\": False\n                }\n\n    model = xgb.XGBRegressor(**param)\n    scores = cross_val_score(model, x_train_2_dropped, y_train_2, scoring=rmsle, cv=5, fit_params=fit_params)\n\n    return scores.mean()\nstudy = optuna.create_study(direction=\"maximize\", study_name=\"original_data_dropped_columns_1\")\nstudy.optimize(objective, n_trials = 300)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = study.best_params\nbest_score = study.best_value\nprint(f\"Best score: {best_score}\\n\")\nprint(f\"Optimized parameters: {best_params}\\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training with best parameters and original+generated data\nparams = {\n        \"objective\": 'reg:squarederror',\n        \"eval_metric\": \"rmsle\",\n        # 'tree_method': 'gpu_hist',\n        \"n_estimators\": 1000,\n        \"early_stopping_rounds\":20,\n        }\n# Add best parameters to params dictionary\nparams.update(best_params)\nreg = xgb.XGBRegressor(**params)\nreg.fit(x_train_2_dropped, y_train_2, eval_set=[(x_test_dropped, y_test)])\nprint(reg)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_test = reg.predict(x_test_dropped).flatten()\nmean_squared_log_error(y_test, y_pred_test, squared=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training with best parameters but only on generated data\nparams = {\n        \"objective\": 'reg:squarederror',\n        \"eval_metric\": \"rmsle\",\n        # 'tree_method': 'gpu_hist',\n        \"n_estimators\": 1000,\n        \"early_stopping_rounds\":20,\n        }\n# Add best parameters to params dictionary\nparams.update(best_params)\nreg2 = xgb.XGBRegressor(**params)\nreg2.fit(x_train_dropped, y_train, eval_set=[(x_test_dropped, y_test)])\nprint(reg2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_test_2 = reg2.predict(x_test_dropped).flatten()\nmean_squared_log_error(y_test, y_pred_test_2, squared=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read data for submission\nsubmit_df = pd.read_csv(testpath)\nsubmit_df.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit_id = submit_df[\"id\"]\n# Predict the cost of the submission data\nsubmit_dropped = submit_df[features].drop(columns=dropping)\ny_pred_submit = reg.predict(submit_dropped).flatten()\nsubmit_final = pd.DataFrame({\"id\": submit_id, \"Class\": y_pred_submit})\n# Save prediction\nsubmit_final.to_csv(outputpath+\"submission3.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_submit = reg2.predict(submit_dropped).flatten()\nsubmit_final = pd.DataFrame({\"id\": submit_id, \"Class\": y_pred_submit})\n# Save prediction\nsubmit_final.to_csv(outputpath+\"submission4.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find good parameters with optuna using only generated data\nimport optuna\nfrom sklearn.model_selection import cross_val_score\nrmsle=make_scorer(mean_squared_log_error, greater_is_better=False, squared=False)\ndef objective(trial):\n\n    param = {\n        \"objective\": 'reg:squarederror',\n        \"eval_metric\": \"rmsle\",\n        'tree_method': 'gpu_hist',\n        \"n_estimators\": 1000,\n        \"early_stopping_rounds\":10,\n        \"verbosity\": 0,\n        \"reg_lambda\": trial.suggest_float(\"lambda\", 0, 100),\n        \"reg_alpha\": trial.suggest_float(\"alpha\", 0, 100),\n        \"max_depth\":  trial.suggest_int(\"max_depth\", 8, 25),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.0001, 0.5, log=True),\n        \"gamma\": trial.suggest_float(\"gamma\", 0, 30),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 100),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.4, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.4, 1.0)\n    }\n\n    fit_params = {\n                \"eval_set\":[(x_test_dropped, y_test)],\n                \"verbose\": False\n                }\n\n    model = xgb.XGBRegressor(**param)\n    scores = cross_val_score(model, x_train_dropped, y_train, scoring=rmsle, cv=5, fit_params=fit_params)\n\n    return scores.mean()\nstudy2 = optuna.create_study(direction=\"maximize\", study_name=\"dropped_columns_2\")\nstudy2.optimize(objective, n_trials = 300)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_params = study2.best_params\nbest_score = study2.best_value\nprint(f\"Best score: {best_score}\\n\")\nprint(f\"Optimized parameters: {best_params}\\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training with best parameters and generated data\nparams = {\n        \"objective\": 'reg:squarederror',\n        \"eval_metric\": \"rmsle\",\n        # 'tree_method': 'gpu_hist',\n        \"n_estimators\": 1000,\n        \"early_stopping_rounds\":20,\n        }\n# Add best parameters to params dictionary\nparams.update(best_params)\nreg3 = xgb.XGBRegressor(**params)\nreg3.fit(x_train_dropped, y_train, eval_set=[(x_test_dropped, y_test)])\nprint(reg3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_test_3 = reg3.predict(x_test_dropped).flatten()\nmean_squared_log_error(y_test, y_pred_test_3, squared=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_submit = reg3.predict(submit_dropped).flatten()\nsubmit_final = pd.DataFrame({\"id\": submit_id, \"Class\": y_pred_submit})\n# Save prediction\nsubmit_final.to_csv(outputpath+\"submission5.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Training with best parameters and original+generated data\nparams = {\n        \"objective\": 'reg:squarederror',\n        \"eval_metric\": \"rmsle\",\n        # 'tree_method': 'gpu_hist',\n        \"n_estimators\": 1000,\n        \"early_stopping_rounds\":20,\n        }\n# Add best parameters to params dictionary\nparams.update(best_params)\nreg4 = xgb.XGBRegressor(**params)\nreg4.fit(x_train_2_dropped, y_train_2, eval_set=[(x_test_dropped, y_test)])\nprint(reg4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_test_4 = reg4.predict(x_test_dropped).flatten()\nmean_squared_log_error(y_test, y_pred_test_4, squared=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_submit = reg4.predict(submit_dropped).flatten()\nsubmit_final = pd.DataFrame({\"id\": submit_id, \"Class\": y_pred_submit})\n# Save prediction\nsubmit_final.to_csv(outputpath+\"submission6.csv\", index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}